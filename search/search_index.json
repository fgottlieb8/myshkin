{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Home","text":""},{"location":"#-dec-23rd-first-post","title":"-[[Dec 23rd - First post]]","text":""},{"location":"Dec%2023rd%20-%20First%20post/","title":"Dec 23rd   First post","text":"<p>I have been focussing on understanding the Bell Inequalities via Classical Causal Models. </p> <p>Causal models rely on simplifying the factorization of a joint probability distributions.</p> <p>Every joint probability distribution admits a factorization like $$p(x_1,x_2,x_3,x_4)=p(x_1|x_2,x_3,x_4)p(x_2|x_3,x_4)p(x_3|x_4)p(x_4)$$Note these factorizations are not unique since you can permute the order of the random variables as you would like. For instance, $$p(x_1,x_2,x_3,x_4)=p(x_4|x_1,x_2,x_3)p(x_1|x_2,x_3)p(x_2|x_3)p(x_3)$$is also a valid factorization. </p> <p>In general we write factorizations as $$p(x_1,x_2,...,x_n)=\\prod_j p(x_{\\pi(j)}|x_{\\pi(1)}x_{\\pi(2)}...x_{\\pi(n-j)})$$for any permutation pi. </p> <p>If we think of the storage of the lookup table needed to store all the information of this joint distribution in general p(x_4) is one D-dimensional distribution (if x_4 can take on D different values). p(x_3|x_4) is a D-dimensional distribution for each possible value of x_4 so we need D^2 values. Taking into account N random variables our look up table grows exponentially in N like O(D^N).</p> <p>Now the idea is that not every probability distribution necessarily needs all N factors. And if we could find a way to write our full distribution as a product of less factors our model would be more efficient and would not need to store O(D^N) values.</p> <p>For instance, we simplify the last two factors if random variable x_3 is unconditionally independent from x_4 i.e. p(x_3|x_4)=p(x_3) so our D^2 sized look up table reduces to a D sized look up table. </p> <p>We can also make a simplification if two random variables are CONDITIONALLY INDEPENDENT. There is also a more subtle case mathematically written as p(x_2|x_3,x_4)=p(x_2|x_4). This means once x_4 is already known, learning anything about x_3 tells us nothing about x_2. So x_2 is independent from x_3 only on the condition that we measured x_4. Note, Its almost like anything x_3 tells about x_2 happens through x_3's influence on x_4.</p> <p>Relation to Bell</p> <p>To understand the Bell inequalities through this frame work lets take the probability distribution $$p_{\\text{general}}(x,y,a,b,\\lambda)=p(x|y,a,b,\\lambda)p(y|a,b,\\lambda)p(a|b,\\lambda)p(b|\\lambda) p(\\lambda)$$ where x and y are the measurement outcomes (up or down) of Alice and Bob respectively. a and b are the choice of measurement basis of Alice and Bob respectively, and lambda is some hidden variable.</p> <p>Now to impose a physical constraint like NO SUPERLUMINAL SIGNALLING, we restrict the factorization of our distribution. </p> <ol> <li>No superdeterminsm: The hidden variable shouldnt control what measurment basis we choose p(a|lambda)=p(a) and p(b|lambda)=p(b) so p(a,b|lambda)=p(a,b)</li> <li>No superluminal causal influence of choice of basis (No signalling): Bobs choice of measurement basis should not effect Alice's measured outcome and vice versa. If it did Alice could communicate to Bob superluminally by choosing her measurement basis. p(x|a,b,lambda)=p(x|a,lambda) and p(y|a,b,lambda)=p(y|b,lambda)</li> <li>No superluminal causal influence of measurement outcomes (The hidden variable is local): Once the hidden variable lambda is known, outcome x should not give us any information about outcome y because this would violate superluminal travel. Einstein wanted these nonlocal correlations to be an illusion that arose from us not knowing the value of this hidden variable and that if we were able to know the hidden variable then the outcome of x and y would be uncorrelated. p(x|y,lambda)=p(x|lambda) and p(y|x,lambda)=p(y|lambda) </li> </ol> <p>All together, Einstein said that if we want to impose no superdeterminism and no signalling and nonlocal correlations in measurement outcomes are explained by local hidden variable our probability distribution of possible outcomes to our bell experiment must be restricted. We can't get any probability distribution because some of those distributions would generally have some nonlocal correlations. Instead the distributions must have a factorization like:</p> <p>$$p_{\\text{einstein}}(x,y,a,b,\\lambda)=p(x|a,\\lambda)p(y|b,\\lambda)p(a,b)p(\\lambda)$$ When we did the experiment the true quantum statistics did not follow Einsteins desired factorization. The experimental statistics followed \"no superdeterminsm\" and \"No superluminal causal influence of choice of basis\" but it did not follow \"local hidden variables\"</p> <p>You can try to patch this error by removing the final constraint i.e. allowing for superluminal causal influence of measurement outcomes while still maintaining the constraint of no signalling. Allegedly this is nonlocal hidden variable theories like Pilot Wave. But I have many questions about what the factorization would be.</p> <p>$$p_{\\text{relaxed}}(x,y,a,b,\\lambda)=p(x|y,a,\\lambda)p(y|b,\\lambda)p(a,b)p(\\lambda)$$ My questions are about: 1. I have heard that there is something fundamentally wrong with applying any of this logic to quantum because it assumes the hidden variable is a scalar lambda instead of a shared quantum state rho. So whenever we condition on quantum distributions we don't just sum over all values of lambda but instead we take the trace. This violates logic like d-seperation of bayesian graphs. Like this would violate the idea of \"knowing lambda will make x independent from y\" because that is an example of d-seperation. What is quantum d-separation? What exactly forces us to treat our random variables as matrices? Can these distributions with matrix random variables be modelled by scalar random variables if we use many more scalar random variables? Like is there a dialation theorem or is it an irrep thing?     1. In my head, this starts to relate to relative facts and no global truth. We say there is no valid joint global probability distribution on the whole quantum system. is this with the inclusion of hidden variables or after having thrown them out?  2. Doesn't this give us our clearest insight to the potential power of quantum objects over classical objects for computation. Quantum objects allow for certain factorizations that classical bits don't. For instance if we did bell experiment with a classical object then it measurement outcomes wouldn't depend on measurement basis. So p(x|a)=p(x) so the a and b random variables essentially become inert and can be removed. So for any joint distribution of N qubits and therefore N measurement basis's the classical alternative will necessarily have less required factors because it doesnt take into account the measurement basis.     1. For instance, as we saw before the look up of a general distribution scales with D^N with N being the number of factors. If we can find a way to use klog(N) factors instead we get a look up table that scales polynomial with O(N^k). this can happen if p(x|a,b,c,d) which naively is D^5 goes to p(x|a,b)p(a)p(b) 2D+D^2 for each random variable. Like we can recursively define a specfic tree structure of dependence that will lead to a reduction of the size of our lookup table. Maybe we can exploit the quantum factorizations because N qubits have a much richer set of correlations due to the measurement outcomes also being random variables.      2. Can we think of solving decision problems as inferring the specific probability distribution that satisfies $$p(x_1,x_2,x_3,x_4,...x_N)=p(\\vec{x})=\\delta_{\\vec{x},\\vec{s}}$$ so its a distribution that takes the entire solution space as input and returns 1 when we find the one point in solution space that gives us the answer. What factorization does this distribution admit? Is it unique? What if there are more than one solution? Instead of thinking of the solution of a bitstring of xs we can think of it as correlations between x_i and x_j. Can we think of probabilistic algorithms as those that approx the delta function as a sharp gaussian so it allows a bit of wiggle room. If correlations is what can help solve the problem does this give us a framework to understand quantum advantage?      3. From my preliminary research SAT problems are about factorizations which are called constraints. I would like to look into this.     4. Theres this idea of factorization relying on conditioning on a variable so measuring a variable. In quantum measurements change the state so the order of measurment matters. However in our probability factorization the order conditioning by definition shouldnt matter because it is built for scalar objects.</p>"}]}